
写入文件

在运行爬虫时,保存在文件中: 
	scrapy crawl ABC -o test.json
	scrapy crawl ABC -o test.csv
下面需要开启settings 的设置
(1)
import json
class Axx(object):
  def open_spider(self,spider):
    file='abc.text' or 'abc.json' or 'xxx'
    self.dilename(file,'w',encoding='uft-8')
  #item为传来的对象，需要转化为字典，若为字典就不需要转了
  def process_item(self,item,spider):
    line=json.dumps(dict(item),ensure_ascii=False)+'\n'
    self.filename.write(line)
    return item
    
   def close_spider(self,spider):
    self.filename.close()
(2)
class Axx(object):
   def process_item(self,item,spider):
    whith open(filename,'a',encoding='utf-8') as f:
      f.write(json.dumps(dict(item),ensure_ascii=False)+'\n')
     return item
     
保存到数据库
(1)mysql
方法1
import MySQLdb 
import pymysql
MySQLdb.connect()
class Axx(object):
  def open_spider(self,spider):
    self.client=connect(host='',port=3306,user='root',password='',db='XX',charset='utf8')
    self.cursor=self.client.cursor()
    
  #item为传来的对象，需要转化为字典，若为字典就不需要转了
  def process_item(self,item,spider):
    sql='insert into xx value(%s,%s....)'
    self.cursor.execute(sql,[item[''],....])
    self.client.commit()
    return item
    
   def close_spider(self,spider):
    self.cursor.close()
    self.client.close()




(2)mongodb
方法1 
from XXX.settings import xxxx
import pymongo
class Axx(object):
  def __init__(self):
    host=''
    port=
    dbname=''
    tb=''
    client=pymongo.MongoClient(host='',port='')
    db=client[dbname]
    self.post=db[tb]
   def process_item(self,item,spider):
    item=dict(item)
    self.post.inset(item)
    return item











      
   
