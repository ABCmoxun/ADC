
写入文件

在运行爬虫时,保存在文件中: 
	scrapy crawl ABC -o test.json
	scrapy crawl ABC -o test.csv
下面需要开启settings 的设置
(1)
import json
class Axx(object):
  def open_spider(self,spider):
    file='abc.text' or 'abc.json' or 'xxx'
    self.dilename(file,'w',encoding='uft-8')
  #item为传来的对象，需要转化为字典，若为字典就不需要转了
  def process_item(self,item,spider):
    line=json.dumps(dict(item),ensure_ascii=False)+'\n'
    self.filename.write(line)
    return item
    
   def close_spider(self,spider):
    self.filename.close()
(2)
class Axx(object):
   def process_item(self,item,spider):
    whith open(filename,'a',encoding='utf-8') as f:
      f.write(json.dumps(dict(item),ensure_ascii=False)+'\n')
     return item
     
保存到数据库
(1)mysql
方法1
 pymysql.connect
import pymysql
MySQLdb.connect()
class Axx(object):
  def open_spider(self,spider):
    self.client= pymysql.connect(host='',port=3306,user='root',password='',db='XX',charset='utf8')
    self.cursor=self.client.cursor()
    self.r= redis.Redis(host='127.0.0.1',port=6379,password='xx',,db=0,,decode_responses=True)
    
    
  #item为传来的对象，需要转化为字典，若为字典就不需要转了
  def process_item(self,item,spider):
    sql='insert into xx value(%s,%s....)'
    self.cursor.execute(sql,[item[''],....])
    self.client.commit()
    return item
    
   def close_spider(self,spider):
    self.cursor.close()
    self.client.close()
  
pool= redis.ConnectionPool(host='localhost',port=6379,decode_responses=True)
r=redis.Redis(connection_pool=pool)




(2)mongodb
方法1 
from XXX.settings import xxxx
import pymongo
class Axx(object):
  def __init__(self):
    host=''
    port=
    dbname=''
    tb=''
    client=pymongo.MongoClient(host='',port='')
    db=client[dbname]
    self.post=db[tb]
   def process_item(self,item,spider):
    item=dict(item)
    self.post.inset(item)
    return item


pipeline中有多个类
用isintance判断是否属于该类
if isinstance(item, JsTotalItem):
	items = dict(item)
if isinstance(item, JsTotalItem):
          items = dict(item)
if spider.name=='xx':
	pass


去重
from scrapy.exceptions import DropItem
class DuplicatesPipeline(object):
    def __init__(self):
        self.ids_seen = set()
    def process_item(self, item, spider):
        if item['id'] in self.ids_seen:
            raise DropItem("Duplicate item found: %s" % item)
        else:
            self.ids_seen.add(item['id'])  ##这里换成你自己的item["#"]
            return item


from scrapy.exceptions import DropItem  #导入异常处理模块
class Baidu03Pipeline(object):
    def __init__(self):          　　#建立构造方法
        self.title = set()       　　#定义集合
    def process_item(self, item, spider):
        title = item['title']    　　#取出要进行判断数据是否重复的字段
        if title in self.title:  　　#如果存在集合中则直接删除该项，
            raise DropItem('{}已存在'.format(title))  
        self.title.add(title)       #如果数据不重复，则加入集合
        ......                      #插库数据 写在这里即可。。。

        return item









