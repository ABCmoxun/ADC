 
 demo.py文件中引进说明，在items.py中可以创建多个类
 from ABC.items import xx,xx
 
 xpath中的 string ,concat
 concat连接字符串
 response.xpath("concat(//div/span/text(),//div/p/text())")
 string提取标签的所有的字符串
 response.xpath("string(//div[@class='xx'])")
 选择器
 
 
 两个层级的网页传递的数据
 item={'':'','':'',}
 yield scrapy.Request(url,callback=self.parsefunc,meta={'item':item})
 
 def parsefunc(self,response):
  item=response.meta['item']
 
 
 
 
 在项目中可以写一个运行文件
 run.py 
 from scrapy import cdmline
 cmdline.execute('scrapy crawl douban_spider'.split())   #官方文件用的这个方法
 #cdmline.execute(['scrapy','crawl','douban_spider'])   等价
 
 
 
 在items文件中有两个或多个类的处理方法
 在pipeline文件中处理
 from jianshutotal.items import AItem, BItem
 查看信息：https://blog.csdn.net/qq_42630844/article/details/82500419
 class JianshutotalPipeline(object):
    def process_item(self, item, spider):
        if isinstance(item, JsTotalItem):
          items = dict(item)



xpath中的兄弟元素

前一位：
preceding-sibling::div[1]
后一位：
following-sibling::div[1]


前N位：
preceding-sibling::div[N]
后N位：
following-sibling::div[N]
'./preceding-sibling::div[1]'
  '//label[contains(text(), "文本内容")]/../following-sibling::div[1]//input'       
 
 
scrapy中的模拟登陆
重写start_requests方法
要使用cookie，第一步得打开它呀，默认scrapy使用CookiesMiddleware中间件，并且打开了。
setting里设置： COOKIES_ENABLES = True  
一. cookies_dict={}
yield scrapy.Request(url,callback=self.parse,cookies=cookies_dict)
2、使用scrapy.FormRequest()登录
通过scrapy.FormRequest能够发送post请求，同时需要添加fromdata参数作为请求体，以及callback
yield scrapy.FormRequest(url,formdata={},callback=self.parse_login）
3、使用scrapy.Formrequest.from_response登陆github
   def parse(self, response):
        yield scrapy.FormRequest.from_response(
            response, # 传入response对象,自动解析
            # 可以通过xpath来定位form表单,当前页只有一个form表单时,将会自动定位
            formxpath='//*[@id="login"]/form', 
            formdata={'login': '****', 'password': '***'},
            callback=self.parse_login)

         

















           
            
            

        
 
 
 
 
 
 
 
 
 
