 
 demo.py文件中引进说明，在items.py中可以创建多个类
 from ABC.items import xx,xx
 
 xpath中的 string ,concat
 concat连接字符串,可以连接两个及以上各
 response.xpath("concat(//div/span/text(),//div/p/text())")
 string提取标签的所有的字符串
 response.xpath("string(//div[@class='xx'])")===response.xpath("//div[@class='xx']//text()")
 选择器
 
 
 两个层级的网页传递的数据
 item={'':'','':'',}
 yield scrapy.Request(url,callback=self.parsefunc,meta={'item':item})
 
 def parsefunc(self,response):
  item=response.meta['item']
 
 
 
 
 在项目中可以写一个运行文件
 run.py 
 from scrapy import cdmline
 cmdline.execute('scrapy crawl douban_spider'.split())   #官方文件用的这个方法
 #cdmline.execute(['scrapy','crawl','douban_spider'])   等价
 
 
 
 在items文件中有两个或多个类的处理方法
 在pipeline文件中处理
 from jianshutotal.items import AItem, BItem
 查看信息：https://blog.csdn.net/qq_42630844/article/details/82500419
 class JianshutotalPipeline(object):
    def process_item(self, item, spider):
        if isinstance(item, JsTotalItem):
          items = dict(item)



xpath中的兄弟元素

前一位：
preceding-sibling::div[1]
后一位：
following-sibling::div[1]

前N位：
preceding-sibling::div[N]
后N位：
following-sibling::div[N]
'./preceding-sibling::div[1]'
  '//label[contains(text(), "文本内容")]/../following-sibling::div[1]//input'       
 
 
scrapy中的模拟登陆
重写start_requests方法
要使用cookie，第一步得打开它呀，默认scrapy使用CookiesMiddleware中间件，并且打开了。
setting里设置： COOKIES_ENABLES = True  
一. cookies_dict={}
yield scrapy.Request(url,callback=self.parse,cookies=cookies_dict)
2、使用scrapy.FormRequest()登录
通过scrapy.FormRequest能够发送post请求，同时需要添加fromdata参数作为请求体，以及callback
yield scrapy.FormRequest(url,formdata={},callback=self.parse_login）
3、使用scrapy.Formrequest.from_response登陆github
   def parse(self, response):
        yield scrapy.FormRequest.from_response(
            response, # 传入response对象,自动解析
            # 可以通过xpath来定位form表单,当前页只有一个form表单时,将会自动定位
            formxpath='//*[@id="login"]/form', 
            formdata={'login': '****', 'password': '***'},
            callback=self.parse_login)

     headers = {
        "Accept":"*/*",
        "Accept-Encoding":"gzip, deflate, sdch",
        "Accept-Language":"zh-CN,zh;q=0.8",
        "Cache-Control":"max-age=0",
        "Connection":"keep-alive",
        "Host": "www.xxxxxx.com",
        "User-Agent":"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36"
    }
--------------------- 
def start_requests(self):
 yield scrapy.Request(url,meta={'cookiejar':1},headers=self.headers, callback=self.parse)



scrapy-splash
https://pypi.org/project/scrapy-splash/

https://splash.readthedocs.io/en/stable/
https://github.com/scrapy-plugins/scrapy-splash

别人的一个项目
https://github.com/playwolf719/EasyCrawler


把json的字符串变成字典
 json.loads(response.text)
 把字典变成json字符串
 json.dumps()








           
            
            

        
 
 
 
 
 
 
 
 
 
