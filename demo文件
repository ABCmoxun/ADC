

scrapy shell url  url可以不加引号
response.text  字符串
response.body   字节流
response.url   response的url


爬虫可以继承多个类
xpath的用法
xpath().extrect()
继续写xpath的话后面可以不.extrect()

'//div[@class]' 带有class属性的div
'//div[last()]' 最后一个div
'//div[last()-1]'；'//div[last()-2]'；'//div[last()-n]'
'//a[contains(@href,'xx')]'  href属性包含 'xx'字符串
xpath('//a[start-with(@href,'xxx')]') 以'xx'字符串开始的href属性的元素
xpath('//a[re:test(@class,正则表达式)]')
xpath('//a[@id="xx"][@class="yy"]//a')两个属性来定位
'//div[@class="xx" and contains(@id,"yy")]' 双属性定位
'//div[@class="xx"][1]'

特别重要
提取字符串
xpath('string(//xxx/div)')
字符串拼接
xpath('concat(//span[@class="xx"]/text(),//span[@class="yy"]/a/text()')

extract()[0]   有时候为空会报错
extract_first()#最好用这个，为空也不会报错
extract_one()






from urllib import urlencode
from scrapy import log
from scrapy.spiders import CrawlSpider
from scrapy.selector import Selector
from scrapy.http import Request, FormRequest
from zhihu.items import ZhihuPeopleItem, ZhihuRelationItem

 yield Request(url,callback=self.parse_people,errback=self.parse_err)
  def parse_people(self, response):
      pass
  def parse_err(self, response):
        log.ERROR('crawl {} failed'.format(response.url))


start_url=[]        #是一个列表
重写start_url方法
def start_requests(self):
 url=''
 yield scrapy.Request(url,callback=self.parse，dont_filter=)
 需要将dont_filter设置为False开启去重，默认是True，没有开启去重；

一些方法
  可以将第一个页面数据传到第二个页面
  yield scrapy.Request(url,callback=self.parse_html,meta={'item':item})
  
  def parse_html(self,response):
    item=response.meta['item']          #使用方法
    
    使用post提交数据
    formdata={}  #一个字典
    yield scrapy.RormRequest(url,formdata=formdata,callback=self.parse_)
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   


        
        
        
        
